{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0f1330",
   "metadata": {},
   "source": [
    "## Llama Index and Llama 2 tutorial on Lonestar 6\n",
    "\n",
    "Llama2 is the Meta open source Large Language Model. LlamaIndex is a python library that connects data to the LLMs such as Llama2. This allows the user to quickly use their unstructured data as a basis for any chats or outputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bbac77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.prompts import PromptTemplate, PromptType\n",
    "from pathlib import Path\n",
    "from llama_index import download_loader, KnowledgeGraphIndex\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1cfb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ecfb9b",
   "metadata": {},
   "source": [
    "## Set your working directory\n",
    "Change your working directory to your Scratch location. This will improve performance, and ensure you have access to the model you rsynced earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e2053af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/06659/wmobley\n",
      "/scratch/06659/wmobley\n"
     ]
    }
   ],
   "source": [
    "scratch = ! echo $SCRATCH\n",
    "os.chdir(scratch[0])\n",
    "! pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a717aa0",
   "metadata": {},
   "source": [
    "## Access the model\n",
    "Next we'll access the models. You have 4 models to access the 7 and 13billion parameters chat and normal model. The folder will also have access to the 70b parameter models; however, we have not tested their performance on the LS6 dev machines. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cddcf1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model names (make sure you have access on HF)\n",
    "LLAMA2_7B = f\"{scratch[0]}/HF/noRef/Llama7b\"\n",
    "LLAMA2_7B_CHAT = f\"{scratch[0]}/HF/noRef/Llama7bchat\"\n",
    "LLAMA2_13B = \"meta-llama/Llama-2-13b-hf\"\n",
    "LLAMA2_13B_CHAT = \"meta-llama/Llama-2-13b-chat-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadbd70f",
   "metadata": {},
   "source": [
    "## Select Model\n",
    "For this script we will chose the Llama 2 13B parameter chat model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ef0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model  =LLAMA2_7B_CHAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437b78f",
   "metadata": {},
   "source": [
    "## Sytem Prompt\n",
    "We are going to have llama2 create triplets used in a knowledge graph based on a pdf. \n",
    "\n",
    "We set up the system prompt below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e81dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT=\"\"\"You are social scientist researcher. You have been tasked with identifying the topics of research papers. \n",
    "\n",
    "Classify the topics of the documents using only the following topics:\n",
    "            \n",
    "## Topics\n",
    "Federal, State, Regional, Local, Operators-in-training, System Operators, Community Members, Not Stage Specific, Source Water, Potable Water Treatment, Potable Water Distribution, Wastewater Collection, Wastewater Treatment, Water Storage, End-Users Storage System, Administrative Processes, Compliance, Strategic Planning, Funding, Worker Safety, Testing, Laboratory, Field, At-home, Effluent at the conclusion of the treatment process, Biological, Chemical, Physical properties, Water received by the end-user, Biological, Chemical, Physical properties, Treatment Process, Equipment Installation, Equipment Operations, Equipment Maintenance, System Components, System Monitoring, Climate and Environment, Cybersecurity, System Breakdown, Hazardous Materials, Background, Application Process, Benefits of Certification, Accessibility of Material, In-Person Training, Library, Online, Cost Considerations, Continuing Education Requirements, Experience Requirements, Previous Education Considerations, Study Tactics, Consequences of Poor Management, Collaboration, Water Governance , Federal policies, regulations , State policies, regulations , Tribal governance , Community Outreach, Water System Stakeholders, Environmental Attorneys, End-Users, Relevance , To environment, To operator, Positive implications, Negative implications, To end user, COVID-19\n",
    "            \n",
    "Here are some rules you always follow:\n",
    "- Do not include numbers before the topics. \n",
    "- Generate only the requested output, don't include any other language before or after the requested output.\n",
    "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
    "- Generate professional language typically used in business documents in North America.\n",
    "- Never generate offensive or foul language.\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f46be",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "Next we'll load the model. If it can't find the model it will download it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1196d17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpnwe30ojv\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpnwe30ojv\n",
      "Created a temporary directory at /tmp/tmpnwe30ojv\n",
      "Created a temporary directory at /tmp/tmpnwe30ojv\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpnwe30ojv/_remote_module_non_scriptable.py\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpnwe30ojv/_remote_module_non_scriptable.py\n",
      "Writing /tmp/tmpnwe30ojv/_remote_module_non_scriptable.py\n",
      "Writing /tmp/tmpnwe30ojv/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd8972f06e54022ab96b3ff94960f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd8972f06e54022ab96b3ff94960f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/06659/wmobley/HF/noRef/Llama7bchat\n",
      "/scratch/06659/wmobley/HF/noRef/Llama7bchat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=2048,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=selected_model,\n",
    "    model_name=selected_model,\n",
    "    device_map=\"auto\",\n",
    "    # change these settings below depending on your GPU\n",
    "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True, \"cache_dir\":f\"{scratch[0]}/HF/noRef\"},\n",
    ")\n",
    "print(selected_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4ec51",
   "metadata": {},
   "source": [
    "## Load the PDF documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2827ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "PDFReader = download_loader(\"PDFReader\")\n",
    "\n",
    "loader = PDFReader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a542906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext, set_global_service_context\n",
    "\n",
    "\n",
    "# Better Explain Each of these steps. \n",
    "topics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cab8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/06659/wmobley/minconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/work/06659/wmobley/minconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/work/06659/wmobley/minconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/work/06659/wmobley/minconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for root, dirs, files in os.walk(f'{scratch[0]}/HF/GrayLit', topdown=False):\n",
    "    for i, name in enumerate(files):\n",
    "        if name.endswith('pdf'):\n",
    "            service_context = ServiceContext.from_defaults(\n",
    "                llm=llm, embed_model=\"local:BAAI/bge-small-en\"\n",
    "            )\n",
    "            set_global_service_context(service_context)\n",
    "            documents = loader.load_data(file=Path(os.path.join(root, name)))\n",
    "            index = VectorStoreIndex.from_documents(documents)\n",
    "            query_engine = index.as_query_engine()\n",
    "            response = query_engine.query(\"Provide a list of Topics\")\n",
    "            if response.response =='Empty Response':\n",
    "                print(name)\n",
    "                continue\n",
    "            response_dict = {name: re.sub('\\d', \"\" , response.response.split(\":\")[1].replace('\\n',\"\")).replace('()',\"\").split(\".\")}\n",
    "            topics.update(response_dict)\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response)\n",
    "import json \n",
    "with open(\"Topics.json\", \"w\") as outfile: \n",
    "    json.dump(topics, outfile)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1790ae3b",
   "metadata": {},
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext, set_global_service_context\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=\"local:BAAI/bge-small-en\"\n",
    ")\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4e1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fbe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What are the topics found in this article?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8511d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document By Document Topic Analysis\n",
    "# Unsupervised\n",
    "## Semi-Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9646d94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
